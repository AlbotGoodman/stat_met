{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deadline: 7 februari.  \n",
    "\n",
    "En hatt på en symbol betyder uppskattning. SSE ska vara så liten som möjligt i minsta kvadratmetod. Confidence and significance (teststatistikor) har vi inte pratat om ännu.  \n",
    "\n",
    "Allt ska göras som en python klass som importeras till jupyter-filen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Föreläsning  \n",
    "\n",
    "## Konfidansintervall  \n",
    "\n",
    "A 100(a-α) % confidence intervall for a parameter θ is a random interval [L1, L2] such that P[L1 <= θ <= L2] = 1-α regardless of the value of θ.  \n",
    "\n",
    "Tänk att vi har ett lasso och ska fånga en pinne. Lassots bredd är mellan L1 och L2, då kommer vi att fånga pinnen (1-α) av gångerna.  \n",
    "\n",
    "I vanliga fall känner vi inte till medlet men om vi hade haft det så hade detta varit enkelt att räkna ut.  \n",
    "\n",
    "Let the following be a random variable: Z = X - µ / (σ / √n). If µ is known then the variable follows a standard normal distribution. Vi kan byta distribution och normalisera distributionen om vi känner till det sanna medlet, inte stickprovets medel. Om vår konfidansnivå är 95 % så ska vi täcka 95 % av kurvan mellan ändpunkterna.  \n",
    "\n",
    "A 100(1-α) % interval on µ (mean?) when σ² (variance) is known. Det betyder att vi kan uppskatta vilken distribution det är.  \n",
    "\n",
    "Let X1, X2, ..., Xn be a random sample from a normal distribution. Då kan vi skriva upp vad konfidensintervallet blir: X /+-/ Z(subscript: α/2) σ √n  \n",
    "\n",
    "T ex:  \n",
    "\n",
    "X = 8  \n",
    "8 /+-/ 2  \n",
    "P[6 <= X <= 10]\n",
    "\n",
    "Alla normaldistributioner är symmetriska, annars är de inte normala.  \n",
    "\n",
    "### Central limit theorem  \n",
    "\n",
    "Let X1, X2, ... , Xn be a random sample of size n from a distribution with mean µ and variance σ². Then, for large n, X is approx normal with mean µ and variance σ². Furthermore X /+-/ Z(subscript: α/2) σ √n  is approx. Standard normal.  \n",
    "\n",
    "Om vi har et stickprov n stort, så är X ungefär normal. Medlet behöver inte vara noll (är den noll så heter det standard normal). Oavsett distribution med tillräckligt många stickprov så kommer den vara ungefär normal och om vi centrerar distributionen genom att dela med σ √n så normaliserar vi den. Oavsett hur komplex distributionen är kan vi antaga och använda våra konfidansintervall som vanligt. Vad är då tillräckligt mycket data? Med stor varians måste vi ha fler testfall men i praktiken räcker det med 20 st testfall. Tänk tärningar, redan efter 20 ggr ser vi en normaldistribution i kurvan (en Bell curve).  \n",
    "\n",
    "Det här är centralt för all applicerad matematik. Tänk att du ser en skog i stället för de individuella träden, en jämn ton i stället för myrornas krig osv.  \n",
    "\n",
    "## Distributioner  \n",
    "\n",
    "Vi börjar med att titta på den geometriska distributionen.  \n",
    "\n",
    "### Geometric distribution  \n",
    "\n",
    "För att den ska vara geometrisk krävs att:  \n",
    "    i. The experiment (hur vi får fram utfallet till våra slumpvariabler) consists of a series of trials. The outcome (utfallet) of each trial is either \"success\" or \"failure\", (s) or (f). Such a trial is known as a Bernoulli trial.  \n",
    "    ii. The trials are identical and independent. (t ex oberoende slantsinglingar)  \n",
    "    iii. The random variable X denotes the number of trials needed to obtain the first success. (Om X beter sig som så att antalet försök för att få det första lyckade försöket - singla klave - asdasdas)  \n",
    "\n",
    "S (utfallsrummet) = {s, fs, ffs, fffs ...}\n",
    "\n",
    "Det är uppräkneligt oändlig mängd vilket för att vara en sannolikhetsdistribution. Det är en diskret distribution när det antingen är sant eller falskt, 1 eller 0. Fuzzy logic är inte så hett längre även om alla ANN använder det.  \n",
    "\n",
    "P[X=1] = probability of success on first trial = p  \n",
    "P[X=2] = probability of failure on first trial and success on the second = (1-p)p   (multiplikationsprincipen??)  \n",
    "P[X=3] = probability of failure on first two trials and success on the third = (1-p)²p    \n",
    "\n",
    "f(X) = (1-p)^x-1 p  (probability function aka täthetsfunktionen?)\n",
    "\n",
    "Summan av alla ska bli ett.  \n",
    "\n",
    "F(x) = 1-q(floor function:x, dvs heltalet avrundat nedåt), q = 1-p     (cumulative distribution function, cdf)  \n",
    "\n",
    "#### Moments for geometric distributions in the population  \n",
    "\n",
    "    E[X] = 1 / p        (väntvärdet, medlet)\n",
    "\n",
    "Om vi vet att det är en geometrisk distribution kan vi enkelt räkna ut väntvärdet.  \n",
    "\n",
    "    VarX = q / p² , q = 1 - p  \n",
    "\n",
    "T ex, om vi singlar slant och variansen är två, då kommer vi mest troligt se klave efter fyra kast.  \n",
    "\n",
    "    σ = √VarX  \n",
    "\n",
    "Den geometriska distributionen är alltså rent visuellt exponentiellt avtagande.  \n",
    "\n",
    "### Binomial distribution  \n",
    "\n",
    "För att distributionen ska vara binomial krävs att:  \n",
    "    i. The experiment consists of a fixed number of Bernoulli trials, with outcomes \"success\" or \"failure\".  \n",
    "    (ii. The trials are identical and independent. (t ex oberoende slantsinglingar)) detta tillhör på sätt och vis också  \n",
    "    ii. The random variable X denotes the number of successes obtained in the n trials.  \n",
    "\n",
    "    Vi gör alltså ett fixerat antal försök och ser hur många gånger vi lyckas.  \n",
    "\n",
    "Example for n = 3:  \n",
    "S = {fff, sff, ssf, sss, sfs, fss, fsf}  \n",
    "\n",
    "fff: (1-p)³  \n",
    "sff: p(1-p)²  \n",
    "...  \n",
    "\n",
    "P[X = x] = C(X) p^x (1-p)^n-x         (C är antalet lyckade försök)\n",
    "C(X) måste alltså vara binomial (n x) hur många set kan vi välja ut lyckade försök.  \n",
    "f(x) = (n x) p^x (1-p)^n-x  \n",
    "\n",
    "x håller reda på hur många lycakde försök av n försök.  \n",
    "\n",
    "F(x) = t∑x=0 (n x) p^x (1-p)^n-x  \n",
    "\n",
    "#### Moments  \n",
    "\n",
    "    E[X] = µ (medlet) = np  \n",
    "    VarX = npq , q = (1-p)  \n",
    "    σ = √VarX  \n",
    "\n",
    "### Probability vs likelihood\n",
    "\n",
    "Sannolikhet och rimlighet. \n",
    "\n",
    "Skillnaden mellan probability and likelihood är \n",
    "\n",
    "Sannolikheten att värdet är två kommer vara väldigt lite. Probability is the value of the probability function (täthetsfunktionen) at that point.  \n",
    "\n",
    "Hur rimligt är det att värdet två tillhör en normaldistribution? Det blir en invers sannolikhet.  \n",
    "\"How likely is the value to belong to a given distribution?\". Svaret vi får säger i någon mening hur långt ifrån medlet vi är.  \n",
    "=> \"more than 1σ (one standard deviation)\" = 68 % likelihood  \n",
    "\n",
    "Vid probability har vi en given kurva och stoppar in värden (för att hitta medel), i likelihood försöker hitta kurvan (hitta distributionen) utifrån våra värden.  \n",
    "\n",
    "Vid linjärnregression gör vi antagandet att distributionen är normal. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
